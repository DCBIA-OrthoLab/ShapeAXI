{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.notebook import tqdm\n",
    "from positional_encodings.torch_encodings import PositionalEncoding2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bbox(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # Extract parameters from the file\n",
    "    bbox = [float(x) for x in lines[0].split()]\n",
    "    return bbox\n",
    "\n",
    "def decompose_pose(pose_matrix):\n",
    "    # Extract rotation (3x3) and translation (3x1)\n",
    "    R = pose_matrix[:3, :3]  # Rotation matrix\n",
    "    t = pose_matrix[:3, 3]   # Translation vector\n",
    "    return R, t\n",
    "\n",
    "def parse_intrinsics(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # Extract parameters from the file\n",
    "    fx = fy = float(lines[0].split()[0])  # Assuming symmetrical focal length\n",
    "    cx = float(lines[0].split()[1])\n",
    "    cy = float(lines[0].split()[2])\n",
    "    width = int(lines[-1].split()[0])\n",
    "    height = int(lines[-1].split()[1])\n",
    "\n",
    "    intrinsics = {\n",
    "        \"fx\": fx,\n",
    "        \"fy\": fy,\n",
    "        \"cx\": cx,\n",
    "        \"cy\": cy,\n",
    "        \"width\": width,\n",
    "        \"height\": height\n",
    "    }\n",
    "    return intrinsics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CameraPoseDataset(Dataset):\n",
    "    def __init__(self, image_dir, pose_dir):\n",
    "        self.image_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "        self.pose_paths = sorted([os.path.join(pose_dir, f) for f in os.listdir(pose_dir) if f.endswith('.txt')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image = torch.from_numpy(np.array(Image.open(self.image_paths[idx])).astype(np.float32) / 255.0)                \n",
    "        image = image.permute(2, 0, 1)[0:3]  # Convert to CxHxW\n",
    "        \n",
    "        # Load pose\n",
    "        with open(self.pose_paths[idx], 'r') as f:            \n",
    "            pose = np.array([list(map(float, line.strip().split())) for line in f.readlines()]).astype(np.float32)            \n",
    "\n",
    "        return image, pose\n",
    "\n",
    "# Usage\n",
    "ds = CameraPoseDataset('/mnt/raid/C1_ML_Analysis/nerf_data/Synthetic_NeRF/Lego/rgb', '/mnt/raid/C1_ML_Analysis/nerf_data/Synthetic_NeRF/Lego/pose')\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsics = parse_intrinsics('/mnt/raid/C1_ML_Analysis/nerf_data/Synthetic_NeRF/Lego/intrinsics.txt')\n",
    "bbox = parse_bbox('/mnt/raid/C1_ML_Analysis/nerf_data/Synthetic_NeRF/Lego/bbox.txt')\n",
    "\n",
    "\n",
    "\n",
    "intrinsics['fx'] = 177.777\n",
    "intrinsics['fy'] = 177.777\n",
    "intrinsics['cx'] = 64\n",
    "intrinsics['cy'] = 64\n",
    "intrinsics['width'] = 128\n",
    "intrinsics['height'] = 128\n",
    "\n",
    "print(intrinsics, bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch = next(iter(dl))\n",
    "images, poses = batch\n",
    "print(images.shape, poses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_camera_positions_plotly(poses, bbox):\n",
    "    cam_positions = []\n",
    "\n",
    "    # Extract camera positions from the poses\n",
    "    for pose in poses:\n",
    "        R, t = decompose_pose(pose)  # Extract rotation and translation\n",
    "        cam_pos = t.cpu().numpy()\n",
    "        cam_positions.append(cam_pos)\n",
    "\n",
    "    cam_positions = torch.stack([torch.tensor(pos) for pos in cam_positions]).numpy()\n",
    "\n",
    "    # Create a 3D scatter plot for camera positions\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=cam_positions[:, 0],\n",
    "        y=cam_positions[:, 1],\n",
    "        z=cam_positions[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=5, color='blue', opacity=0.8),\n",
    "        name=\"Camera Positions\"\n",
    "    ))\n",
    "\n",
    "    # Add bounding box\n",
    "    x_min, y_min, z_min, x_max, y_max, z_max, _ = bbox\n",
    "\n",
    "    # Define the 8 corners of the bounding box\n",
    "    corners = np.array([\n",
    "        [x_min, y_min, z_min],\n",
    "        [x_max, y_min, z_min],\n",
    "        [x_max, y_max, z_min],\n",
    "        [x_min, y_max, z_min],\n",
    "        [x_min, y_min, z_max],\n",
    "        [x_max, y_min, z_max],\n",
    "        [x_max, y_max, z_max],\n",
    "        [x_min, y_max, z_max]\n",
    "    ])\n",
    "\n",
    "    # Define edges (pairs of corners)\n",
    "    edges = [\n",
    "        (0, 1), (1, 2), (2, 3), (3, 0),  # Bottom face\n",
    "        (4, 5), (5, 6), (6, 7), (7, 4),  # Top face\n",
    "        (0, 4), (1, 5), (2, 6), (3, 7)   # Vertical edges\n",
    "    ]\n",
    "\n",
    "    # Draw edges of the bounding box\n",
    "    for edge in edges:\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=[corners[edge[0], 0], corners[edge[1], 0]],\n",
    "            y=[corners[edge[0], 1], corners[edge[1], 1]],\n",
    "            z=[corners[edge[0], 2], corners[edge[1], 2]],\n",
    "            mode='lines',\n",
    "            line=dict(color='green', width=3),\n",
    "            showlegend=False\n",
    "        ))\n",
    "\n",
    "    # Draw corner points\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=corners[:, 0],\n",
    "        y=corners[:, 1],\n",
    "        z=corners[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=5, color='green'),\n",
    "        name=\"Bounding Box Corners\"\n",
    "    ))\n",
    "\n",
    "    return fig\n",
    "\n",
    "def visualize_rays_plotly(ray_origins, ray_directions, poses, bbox, num_rays=50, ray_length=1.0, pts=None):\n",
    "    \"\"\"\n",
    "    Visualizes camera rays and camera planes using Plotly.\n",
    "\n",
    "    Args:\n",
    "        ray_origins: Tensor of shape [batch_size, num_rays_total, 3]\n",
    "        ray_directions: Tensor of shape [batch_size, num_rays_total, 3]\n",
    "        poses: Tensor of shape [batch_size, 4, 4] containing camera poses.        \n",
    "        num_rays: Number of rays to sample per camera\n",
    "        ray_length: Length of the rays in visualization\n",
    "\n",
    "    Returns:\n",
    "        Interactive 3D Plotly figure\n",
    "    \"\"\"\n",
    "    batch_size, num_rays_total, _ = ray_origins.shape\n",
    "\n",
    "    fig = visualize_camera_positions_plotly(poses, bbox)\n",
    "\n",
    "    # Process each camera in the batch\n",
    "    for cam_idx in range(batch_size):\n",
    "        # Extract camera rotation and translation\n",
    "        R, t = poses[cam_idx, :3, :3], poses[cam_idx, :3, 3]\n",
    "        cam_pos = t.cpu().numpy()  # Compute world-space camera position\n",
    "\n",
    "        # Sample a subset of rays for visualization\n",
    "        sampled_indices = torch.randperm(num_rays_total)[:num_rays]\n",
    "        sampled_origins = ray_origins[cam_idx, sampled_indices].cpu().numpy()\n",
    "        sampled_directions = ray_directions[cam_idx, sampled_indices].cpu().numpy()\n",
    "\n",
    "        # Compute ray endpoints\n",
    "        sampled_endpoints = sampled_origins + ray_length * sampled_directions\n",
    "\n",
    "        # Add rays to the plot\n",
    "        for i in range(num_rays):\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=[sampled_origins[i, 0], sampled_endpoints[i, 0]],\n",
    "                y=[sampled_origins[i, 1], sampled_endpoints[i, 1]],\n",
    "                z=[sampled_origins[i, 2], sampled_endpoints[i, 2]],\n",
    "                mode='lines',\n",
    "                line=dict(color='red', width=2),\n",
    "                name=f\"Ray {i}\" if cam_idx == 0 and i == 0 else None  # Show legend only once\n",
    "            ))\n",
    "\n",
    "        if pts is not None:\n",
    "            p = pts[cam_idx, sampled_indices].cpu().numpy()\n",
    "            p = p.reshape(-1, 3)\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=p[:, 0],\n",
    "                y=p[:, 1],\n",
    "                z=p[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(size=2, color='blue')\n",
    "            ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"3D Visualization of Camera Rays & Planes\",\n",
    "        scene=dict(\n",
    "            xaxis_title=\"X\",\n",
    "            yaxis_title=\"Y\",\n",
    "            zaxis_title=\"Z\",\n",
    "            aspectmode=\"auto\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    def __init__(self):\n",
    "        self.fx = 177.777\n",
    "        self.fy = 177.777\n",
    "        self.cx = 64\n",
    "        self.cy = 64\n",
    "        self.width = 128\n",
    "        self.height = 128\n",
    "\n",
    "class Ray():\n",
    "    def __init__(self):\n",
    "        self.hparams = Params()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def generate_rays(self, c2w):\n",
    "        \"\"\"\n",
    "        Generate rays for a given camera configuration.\n",
    "\n",
    "        Args:\n",
    "            c2w: Camera-to-world transformation matrix (4x4).\n",
    "\n",
    "        Returns:\n",
    "            rays_o: Ray origins (H*W, 3).\n",
    "            rays_d: Ray directions (H*W, 3).\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = c2w.shape[0]\n",
    "        device = self.device  # Get the device of c2w\n",
    "        focal = self.hparams.fx\n",
    "        W = self.hparams.width\n",
    "        H = self.hparams.height\n",
    "        # print(type(H), type(W), type(focal), type(c2w))\n",
    "\n",
    "        i, j = torch.meshgrid(\n",
    "            torch.arange(W, dtype=torch.float32, device=device),\n",
    "            torch.arange(H, dtype=torch.float32, device=device),\n",
    "            indexing='xy'\n",
    "        )\n",
    "        dirs = torch.stack(\n",
    "            [(i - W * .5) / focal, -(j - H * .5) / focal, -torch.ones_like(i, device = device)], -1\n",
    "        ).unsqueeze(0).repeat(batch_size, 1, 1, 1)\n",
    "        \n",
    "        rays_d = torch.bmm(dirs.view(batch_size, -1, 3), c2w[:, :3, :3].transpose(-1, -2))\n",
    "        \n",
    "        rays_o = c2w[:, :3, -1].unsqueeze(1).expand(rays_d.shape)\n",
    "\n",
    "        return rays_o, rays_d\n",
    "\n",
    "# Generate rays\n",
    "\n",
    "ray_origins, ray_directions = Ray().generate_rays(poses.cuda())\n",
    "\n",
    "fig = visualize_rays_plotly(ray_origins, ray_directions, poses, bbox, num_rays=20, ray_length=1.0)\n",
    "fig.show()\n",
    "\n",
    "# torch.Size([2, 128, 128, 1, 3]) torch.Size([2, 3, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRF(nn.Module):\n",
    "  def __init__(self, input_dim=3, pos_dim=60, view_dim=24, hidden=256) -> None:\n",
    "      super().__init__()\n",
    "\n",
    "      self.proj_pos = nn.Linear(input_dim, pos_dim)\n",
    "      self.pos_enc_pos = PositionalEncoding2D(pos_dim)\n",
    "      self.act = nn.ReLU()\n",
    "\n",
    "      self.block1 = nn.Sequential(nn.Linear(pos_dim, hidden), \n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden, hidden),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden, hidden),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden, hidden),\n",
    "      nn.ReLU())\n",
    "      \n",
    "      self.block2 = nn.Sequential(nn.Linear(pos_dim + hidden, hidden),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden, hidden),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden, hidden),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden, hidden)) # No activation\n",
    "      \n",
    "      self.proj_view = nn.Linear(input_dim, view_dim)\n",
    "      self.pos_enc_view = PositionalEncoding2D(view_dim)\n",
    "\n",
    "      self.final_sigma = nn.Sequential(nn.Linear(view_dim + hidden, 1), \n",
    "      nn.ReLU(),\n",
    "      nn.Linear(1, 1),\n",
    "      nn.Softplus())\n",
    "\n",
    "      self.final_rgb = nn.Sequential(nn.Linear(view_dim + hidden, hidden),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden, 3),\n",
    "      nn.Sigmoid())\n",
    "\n",
    "  def forward(self, x_p, x_v):\n",
    "\n",
    "      # parameters:\n",
    "      # x_p: torch.Size([4, N_P, N_Samples, 3]) N_P is the number of points, N_Samples is the number of samples/bins\n",
    "      # x_v: torch.Size([4, N_P, N_Samples, 3]) N_V is the number of view directions\n",
    "\n",
    "      x_p_pos_enc = self.proj_pos(x_p)\n",
    "      x_p_pos_enc = self.pos_enc_pos(x_p_pos_enc)\n",
    "      x_p = self.act(x_p_pos_enc)\n",
    "\n",
    "      x_p = self.block1(x_p)\n",
    "\n",
    "      x_p = torch.cat([x_p, x_p_pos_enc], dim=-1)\n",
    "\n",
    "      x_p = self.block2(x_p)\n",
    "\n",
    "      x_v = self.proj_view(x_v)\n",
    "      x_v = self.pos_enc_view(x_v)\n",
    "\n",
    "      x = torch.cat([x_p, x_v], dim=-1)\n",
    "\n",
    "      sigma = self.final_sigma(x)\n",
    "      rgb = self.final_rgb(x)\n",
    "\n",
    "      return rgb, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accumulated_transmittance(alphas):\n",
    "    accumulated_transmittance = torch.cumprod(alphas, 1)\n",
    "    return torch.cat((torch.ones((accumulated_transmittance.shape[0], 1), device=alphas.device),\n",
    "                      accumulated_transmittance[:, :-1]), dim=-1)\n",
    "\n",
    "# def render_rays(network_fn, rays_o, rays_d, near, far, nb_bins, device, rand=False):\n",
    "\n",
    "#     # Sampling\n",
    "#     z_vals = torch.linspace(near, far, steps=nb_bins, device=device)\n",
    "\n",
    "#     if rand:\n",
    "#         z_vals += torch.rand(*z_vals.shape[:-1], nb_bins, device=rays_o.device) * (far - near) / nb_bins\n",
    "\n",
    "#     pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n",
    "\n",
    "#     # Normalize view directions\n",
    "#     view_dirs = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "#     view_dirs = view_dirs[..., None, :].expand(pts.shape)\n",
    "    \n",
    "#     rgb, sigma = network_fn(pts, view_dirs)\n",
    "    \n",
    "#     # Improved volume rendering\n",
    "#     dists = z_vals[..., 1:] - z_vals[..., :-1]  # Shape: [batch, N_samples-1]\n",
    "#     dists = torch.cat([dists, torch.tensor([1e10], device=device)], -1)\n",
    "    \n",
    "#     # No need to manually expand dists as broadcasting will handle it\n",
    "#     alpha = 1. - torch.exp(-sigma.squeeze(-1) * dists)  # Shape: [batch, N_samples]\n",
    "#     alpha = alpha.unsqueeze(-1)  # Shape: [batch, N_samples, 1]\n",
    "    \n",
    "#     # Computing transmittance\n",
    "#     ones_shape = (alpha.shape[0], alpha.shape[1], 1, 1)\n",
    "    \n",
    "#     T = torch.cumprod(\n",
    "#         torch.cat([\n",
    "#             torch.ones(ones_shape, device=device),\n",
    "#             1. - alpha + 1e-10\n",
    "#         ], dim=2),\n",
    "#         dim=2\n",
    "#     )[:,:,:-1]  # Shape: [batch, N_samples, 1]\n",
    "\n",
    "#     weights = alpha * T  # Shape: [batch, N_samples, 1]\n",
    "\n",
    "#     # Compute final colors and depths\n",
    "#     rgb_map = torch.sum(weights * rgb, dim=2)  # Sum along sample dimension\n",
    "#     depth_map = torch.sum(weights.squeeze(-1) * z_vals, dim=-1)  # Shape: [batch]\n",
    "#     acc_map = torch.sum(weights.squeeze(-1), dim=-1)  # Shape: [batch]\n",
    "\n",
    "#     return rgb_map, depth_map, acc_map\n",
    "\n",
    "\n",
    "def render_rays(nerf_model, rays_o, rays_d, rand=False, device='cpu', near=1.0, far=6.0, nb_bins=64):\n",
    "\n",
    "    # Sampling z_vals is t in the paper\n",
    "    z_vals = torch.linspace(near, far, steps=nb_bins, device=device)\n",
    "\n",
    "    if rand:\n",
    "        z_vals += torch.rand(*z_vals.shape[:-1], nb_bins, device=device) * (far - near) / nb_bins\n",
    "\n",
    "\n",
    "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n",
    "    \n",
    "    dirs = rays_d.unsqueeze(-2).expand(pts.shape)\n",
    "    \n",
    "    rgb, sigma = nerf_model(pts, dirs)\n",
    "\n",
    "    delta = torch.diff(z_vals, dim=-1)\n",
    "    delta = torch.cat([delta, torch.tensor([1e10], device=device)], dim=-1)\n",
    "    \n",
    "    alpha = 1.0 - torch.exp(-sigma.squeeze(-1) * delta)\n",
    "    cumprod = torch.cumprod(1.0 - alpha + 1e-10, dim=2)\n",
    "    exclusive_cumprod = torch.cat([torch.ones_like(alpha[..., :1]), cumprod[..., :-1]], dim=-1)\n",
    "    \n",
    "    weights = (alpha * exclusive_cumprod)\n",
    "\n",
    "    rgb_map = (weights.unsqueeze(-1) * rgb).sum(dim=2)\n",
    "    depth_map = (weights*z_vals).sum(dim=2)\n",
    "    acc_map = weights.sum(dim=2)\n",
    "\n",
    "    return rgb_map, depth_map, acc_map, pts\n",
    "\n",
    "nerf_model = NeRF().cuda()\n",
    "rgb, depth, acc, pts = render_rays(nerf_model, ray_origins.cuda(), ray_directions.cuda(), rand=False, device='cuda:0', near=1.0, far=6.0, nb_bins=64) \n",
    "\n",
    "fig = visualize_rays_plotly(ray_origins, ray_directions, poses, bbox, num_rays=20, ray_length=1.0, pts=pts)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nerf_model, optimizer, scheduler, data_loader, intrinsics, device='cpu', near=0, far=1, nb_epochs=int(1e5), nb_bins=64, NSamples=128*128):\n",
    "    training_loss = []\n",
    "    for _ in tqdm(range(nb_epochs)):\n",
    "        for batch in data_loader:\n",
    "            \n",
    "            images, poses = batch\n",
    "            images = images.to(device)\n",
    "            poses = poses.to(device)\n",
    "            \n",
    "            ray_origins, ray_directions = generate_rays(poses, intrinsics)\n",
    "\n",
    "            idx_samples = torch.randint(0, ray_origins.shape[1], (NSamples,), device=device)\n",
    "            \n",
    "            ray_origins = ray_origins[:, idx_samples]\n",
    "            ray_directions = ray_directions[:, idx_samples]\n",
    "\n",
    "            images = images.view(images.shape[0], images.shape[1], -1)            \n",
    "            images = images[:, :, idx_samples]\n",
    "\n",
    "            # rgb = []\n",
    "\n",
    "            # for r_o, r_d in zip(torch.chunk(ray_origins, chunks=128, dim=1), torch.chunk(ray_directions, chunks=128, dim=1)):\n",
    "            #     rgb_, _, _ = render_rays(nerf_model, r_o, r_d, device=device, near=near, far=far, nb_bins=nb_bins)            \n",
    "            #     rgb.append(rgb_)\n",
    "\n",
    "            # rgb = torch.cat(rgb, dim=1)\n",
    "\n",
    "            rgb, _, _ = render_rays(nerf_model, ray_origins, ray_directions, device=device, near=near, far=far, nb_bins=nb_bins)            \n",
    "            \n",
    "            loss = ((images - rgb.permute(0, 2, 1)) ** 2).sum()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss.append(loss.item())\n",
    "        scheduler.step()\n",
    "\n",
    "        # for img_index in range(200):\n",
    "        #     test(hn, hf, testing_dataset, img_index=img_index, nb_bins=nb_bins, H=H, W=W)\n",
    "\n",
    "    return training_loss\n",
    "\n",
    "\n",
    "nerf_model = NeRF().cuda()\n",
    "optimizer = torch.optim.Adam(nerf_model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "\n",
    "# train(nerf_model=nerf_model, optimizer=optimizer, scheduler=scheduler, data_loader=dl, intrinsics=intrinsics, device='cuda', near=0.8, far=4, nb_epochs=10, nb_bins=64, NSamples=128*128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def test(hn, hf, dataset, chunk_size=10, img_index=0, nb_bins=192, H=400, W=400):\n",
    "    ray_origins = dataset[img_index * H * W: (img_index + 1) * H * W, :3]\n",
    "    ray_directions = dataset[img_index * H * W: (img_index + 1) * H * W, 3:6]\n",
    "\n",
    "    data = []\n",
    "    for i in range(int(np.ceil(H / chunk_size))):\n",
    "        ray_origins_ = ray_origins[i * W * chunk_size: (i + 1) * W * chunk_size].to(device)\n",
    "        ray_directions_ = ray_directions[i * W * chunk_size: (i + 1) * W * chunk_size].to(device)\n",
    "\n",
    "        regenerated_px_values = render_rays(model, ray_origins_, ray_directions_, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "        data.append(regenerated_px_values)\n",
    "    img = torch.cat(data).data.cpu().numpy().reshape(H, W, 3)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.savefig(f'novel_views/img_{img_index}.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe2 = PositionalEncoding2D(128)\n",
    "\n",
    "pe2(torch.rand(4, 1024, 64, 128)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(x, L=10):\n",
    "    rets = [x]\n",
    "    for i in range(L):\n",
    "        for fn in [torch.sin, torch.cos]:  # Use torch functions\n",
    "            rets.append(fn(2. ** i * x))\n",
    "    return torch.cat(rets, dim=-1)  # Concatenate along the last dimension\n",
    "\n",
    "class NeRF_v2(nn.Module):\n",
    "\n",
    "  def __init__(self, pos_enc_dim = 63, view_enc_dim = 27, hidden = 256) -> None:\n",
    "     super().__init__()\n",
    "\n",
    "     self.linear1 = nn.Sequential(nn.Linear(pos_enc_dim,hidden),nn.ReLU())\n",
    "\n",
    "     self.pre_skip_linear = nn.Sequential()\n",
    "     for _ in range(4):\n",
    "      self.pre_skip_linear.append(nn.Linear(hidden,hidden))\n",
    "      self.pre_skip_linear.append(nn.ReLU())\n",
    "\n",
    "\n",
    "     self.linear_skip = nn.Sequential(nn.Linear(hidden+pos_enc_dim,hidden),nn.ReLU())\n",
    "\n",
    "     self.post_skip_linear = nn.Sequential()\n",
    "     for _ in range(2):\n",
    "      self.post_skip_linear.append(nn.Linear(hidden,hidden))\n",
    "      self.post_skip_linear.append(nn.ReLU())\n",
    "\n",
    "     self.density_layer = nn.Sequential(nn.Linear(hidden,1), nn.ReLU())\n",
    "\n",
    "     self.linear2 = nn.Linear(hidden,hidden)\n",
    "\n",
    "     self.color_linear1 = nn.Sequential(nn.Linear(hidden+view_enc_dim,hidden//2),nn.ReLU())\n",
    "     self.color_linear2 = nn.Sequential(nn.Linear(hidden//2, 3),nn.Sigmoid())\n",
    "\n",
    "     self.relu = nn.ReLU()\n",
    "     self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "  def forward(self,input):\n",
    "\n",
    "    # Extract pos and view dirs\n",
    "    positions = input[..., :3]\n",
    "    view_dirs = input[...,3:]\n",
    "\n",
    "    # Encode\n",
    "    pos_enc = encoding(positions,L=10)\n",
    "    view_enc = encoding(view_dirs, L=4)\n",
    "\n",
    "    x = self.linear1(pos_enc)\n",
    "    x = self.pre_skip_linear(x)\n",
    "\n",
    "    # Skip connection\n",
    "    x = torch.cat([x, pos_enc], dim=-1)\n",
    "    x = self.linear_skip(x)\n",
    "\n",
    "    x = self.post_skip_linear(x)\n",
    "\n",
    "    # Density prediction\n",
    "    sigma = self.density_layer(x)\n",
    "\n",
    "    x = self.linear2(x)\n",
    "\n",
    "    # Incoroporate view encoding\n",
    "    x = torch.cat([x, view_enc],dim=-1)\n",
    "    x = self.color_linear1(x)\n",
    "\n",
    "    # Color Prediction\n",
    "    rgb = self.color_linear2(x)\n",
    "\n",
    "    return torch.cat([sigma, rgb], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeRF_v2()\n",
    "model(torch.rand(1, 64, 6)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(x, L=10):\n",
    "    # x is assumed to be a tensor with shape (..., D)\n",
    "    device, dtype = x.device, x.dtype\n",
    "    # Precompute the frequency factors: shape (L,)\n",
    "    freqs = (2 ** torch.arange(L, device=device, dtype=dtype)) * torch.pi\n",
    "    # Expand dimensions of x to (..., D, 1) for broadcasting\n",
    "    x_expanded = x.unsqueeze(-1)\n",
    "    # Compute the arguments for sin and cos: shape (..., D, L)\n",
    "    args = x_expanded * freqs\n",
    "    # Compute sin and cos encodings: shape (..., D, L) each\n",
    "    sin_enc = torch.sin(args)\n",
    "    cos_enc = torch.cos(args)\n",
    "    # Interleave the sin and cos encodings:\n",
    "    # First stack to get shape (..., D, L, 2) then flatten the last two dimensions to (..., D, 2*L)\n",
    "    sin_cos = torch.stack((sin_enc, cos_enc), dim=-1).view(*x.shape[:-1], -1)\n",
    "    # Concatenate the original input with the positional encodings along the last dimension\n",
    "    return torch.cat((x, sin_cos), dim=-1)\n",
    "\n",
    "\n",
    "x = torch.rand(4, 4, 4, 3)\n",
    "x_1 = encoding(x)\n",
    "\n",
    "def encoding_v2(x, L=10):\n",
    "    res = [x]\n",
    "    for i in range(L):\n",
    "        for fn in [torch.sin, torch.cos]:\n",
    "            res.append(fn(2 ** i * torch.pi * x))\n",
    "\n",
    "    print(len(res), res[0].shape, res[1].shape, res[2].shape)\n",
    "    return torch.cat(res,dim=-1)\n",
    "\n",
    "x_2 = encoding_v2(x)\n",
    "\n",
    "(x_1 - x_2).abs().max() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.load('/mnt/raid/C1_ML_Analysis/nerf_data/tiny_nerf_data.npz')\n",
    "# images = data['images']\n",
    "# poses = data['poses']\n",
    "# focal = data['focal']\n",
    "# H, W = images.shape[1:3]\n",
    "# print(images.shape, poses.shape, focal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
