{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shapeaxi.saxi_nets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     20\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/raid/C1_ML_Analysis/source/ShapeAXI\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshapeaxi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshapeaxi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaxi_layers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeDistributed, MHA, Residual, FeedForward, UnpoolMHA, SmoothAttention, SmoothMHA\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshapeaxi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaxi_nets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SaxiMHAEncoder, SaxiMHADecoder\n",
      "File \u001b[0;32m/mnt/raid/C1_ML_Analysis/source/ShapeAXI/shapeaxi/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaxi_nets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DentalModelSeg\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saxi_layers\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shapeaxi.saxi_nets'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from pytorch3d.io import load_obj, save_obj\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.utils import ico_sphere\n",
    "from pytorch3d.ops import sample_points_from_meshes, knn_points, knn_gather\n",
    "from pytorch3d.loss import (\n",
    "    chamfer_distance, \n",
    "    mesh_edge_loss, \n",
    "    mesh_laplacian_smoothing, \n",
    "    mesh_normal_consistency,\n",
    ")\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import sys\n",
    "sys.path.append('/mnt/raid/C1_ML_Analysis/source/ShapeAXI')\n",
    "from shapeaxi import utils\n",
    "from shapeaxi.saxi_layers import TimeDistributed, MHA, Residual, FeedForward, UnpoolMHA, SmoothAttention, SmoothMHA\n",
    "from shapeaxi.saxi_nets import SaxiMHAEncoder, SaxiMHADecoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fn = '/mnt/famli_netapp_shared/C1_ML_Analysis/src/diffusion-models/blender/studies/placenta/FAM-025-0499-5/brain/leftWhiteMatter.stl'\n",
    "target = utils.ReadSurf(target_fn)\n",
    "target, target_mean_bb, target_scale_factor = utils.ScaleSurf(target)\n",
    "target_v, target_f, target_e = utils.PolyDataToTensors(target, device=device)\n",
    "target_mesh = Meshes(verts=[target_v], faces=[target_f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pointcloud(mesh, title=\"\"):\n",
    "    points = sample_points_from_meshes(mesh, 5000)\n",
    "    x, y, z = points.clone().detach().cpu().squeeze().unbind(1)    \n",
    "    fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z)])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_levels = []\n",
    "sample_levels_faces = []\n",
    "for l in range(5, 2, -1):\n",
    "    ico_s = utils.IcoSphere(l)\n",
    "    source_v, source_f = utils.PolyDataToTensors_v_f(ico_s)\n",
    "    sample_levels.append(len(source_v))\n",
    "    sample_levels_faces.append(len(source_f))\n",
    "print(sample_levels)\n",
    "print(sample_levels[::-1])\n",
    "print(sample_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SaxiMHAEncoder()\n",
    "model = model.to(device)\n",
    "target_mesh = Meshes(verts=[target_v.cuda()], faces=[target_f.cuda()])\n",
    "\n",
    "X, X_N = sample_points_from_meshes(target_mesh, sample_levels[0], return_normals=True)\n",
    "X = torch.cat([X, X_N], dim=1)\n",
    "target_mesh_encoded, _ = model(X.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoder = SaxiMHADecoder(input_dim=256)\n",
    "decoder = decoder.cuda()\n",
    "X_hat = decoder(target_mesh_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mesh.verts_padded().shape\n",
    "target_mesh.verts_list()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def saxi_point_triangle_distance(X, X_hat, K_triangle=1, ignore_first=False, randomize=False):\n",
    "    \"\"\"\n",
    "    Compute the distance between a point and the nearest triangle.\n",
    "    It uses the knn_points and knn_gather functions from PyTorch3D to find the nearest triangle.\n",
    "    Args:\n",
    "        X: (B, N0, 3) tensor of points\n",
    "        X_hat: (B, N1, 3) tensor of points\"\"\"\n",
    "    \n",
    "    k_ignore = 0\n",
    "    if ignore_first:\n",
    "        k_ignore = 1\n",
    "\n",
    "    dists = knn_points(X_hat, X, K=(3*K_triangle + k_ignore))\n",
    "    start_idx = (3*(K_triangle-1)) + k_ignore\n",
    "\n",
    "    if randomize:\n",
    "        idx = dists.idx[:, :, torch.randperm(dists.idx.shape[2])]\n",
    "    else:\n",
    "        idx = dists.idx\n",
    "    \n",
    "    x = knn_gather(X, idx[:, :, start_idx:start_idx + 3])\n",
    "    # Compute the normal of the triangle\n",
    "    \n",
    "    N = torch.cross(x[:, :, 1] - x[:, :, 0], x[:, :, 2] - x[:, :, 0], dim=-1)\n",
    "    N = N / torch.norm(N, dim=1, keepdim=True)\n",
    "    # Compute the vector from the point to the first vertex of the triangle\n",
    "    X_v = (X_hat - x[:, :, 0]) \n",
    "    \n",
    "    return torch.sum(torch.abs(torch.einsum('ijk,ijk->ij', X_v, N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saxi_point_triangle_distance(target_v.unsqueeze(0), X_hat, K_triangle=3, randomize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saxi_point_triangle_distance(target_v.unsqueeze(0), X_hat, ignore_first=True, K_triangle=3, randomize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = np.array([2, 3, 4, 4])\n",
    "\n",
    "test_v = torch.range(start=0, end=np.prod(size) - 1).reshape(size.tolist())\n",
    "test_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_v.view(test_v.shape[0], -1, test_v.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapeaxi.saxi_layers import SelfAttention\n",
    "\n",
    "# Example shapes\n",
    "BS = 2  # Batch size\n",
    "V_n = 1000  # Some dimension\n",
    "K = 4  # Number of neighbors\n",
    "embed_dim = 128  # Embedding dimension\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, pooling_factor=0.5, embed_dim=128, hidden_dim=64, K=4):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.pooling_factor = pooling_factor\n",
    "        self.embed_dim = embed_dim\n",
    "        self.attn = SelfAttention(embed_dim, hidden_dim, dim=2)\n",
    "        self.K = K\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # find closest points to self, i.e., each point in the sample finds the closest K points in the sample\n",
    "        dists = knn_points(x, x, K=self.K)\n",
    "        # gather the K closest points\n",
    "        \n",
    "        x = knn_gather(x, dists.idx)\n",
    "        # apply self attention, i.e., weighted average of the K closest points\n",
    "        x, x_s = self.attn(x, x)\n",
    "        x_s = x_s[:,:,0,:]\n",
    "\n",
    "        n_samples = int(x.shape[1]*self.pooling_factor)\n",
    "        idx = torch.argsort(x_s, descending=True, dim=1)[:,:n_samples]\n",
    "        \n",
    "        x = knn_gather(x, idx).squeeze(2)\n",
    "        x_s = knn_gather(x_s, idx).squeeze(2)\n",
    "        \n",
    "        return x, x_s\n",
    "\n",
    "x = torch.rand(BS, V_n, embed_dim)\n",
    "AttentionPooling(pooling_factor=0.25)(x)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
